{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for handling and parsing email files\n",
    "import email\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00001.1a31cc283af0060967a233d26548a6ce\n",
      "0000.7b1b73cf36cf9dbc3d64e3f2ee2b91f1\n"
     ]
    }
   ],
   "source": [
    "# Load files by creating lists of file names in resources files\n",
    "ham_fnames = [name for name in sorted(os.listdir(\"Resources/main_ham\"))]\n",
    "spam_fnames = [name for name in sorted(os.listdir(\"Resources/main_spam\"))]\n",
    "\n",
    "#Return file names\n",
    "print(ham_fnames[0])\n",
    "print(spam_fnames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<email.message.Message at 0x21fa0c09540>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a function to create lists containing email objects\n",
    "def parse_email(fname, spam=False):\n",
    "    directory = \"Resources/main_spam\" if spam else \"Resources/main_ham\"\n",
    "    with open(os.path.join(directory, fname), \"rb\") as fp:\n",
    "        return email.parser.BytesParser().parse(fp)\n",
    "        \n",
    "# Call the function on both sets of data  \n",
    "ham_emails = [parse_email(name) for name in ham_fnames]\n",
    "spam_emails = [parse_email(name, spam=True) for name in spam_fnames]\n",
    "\n",
    "# Return object\n",
    "ham_emails[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a helper function that converts html emails to plain text\n",
    "def html_to_text(email) -> str:\n",
    "    try:\n",
    "        soup = BeautifulSoup(email.get_payload(), \"html.parser\")\n",
    "        plain = soup.text.replace(\"=\\n\", \"\")\n",
    "        plain = re.sub(r\"\\s+\", \" \", plain)\n",
    "        return plain.strip()\n",
    "    except:\n",
    "        return \"nothing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> From:  Valdis.Kletnieks@vt.edu\n",
      "> Date:  Wed, 21 Aug 2002 02:36:56 -0400\n",
      ">\n",
      "> --==_Exmh_778588528P\n",
      "> Content-Type: text/plain; charset=us-ascii\n",
      "> \n",
      "> On Tue, 20 Aug 2002 22:51:52 EDT, Valdis.Kletnieks@vt.edu said:\n",
      "> \n",
      "> > Ever tried to get MH to *not* have a 'pseq' sequence?  I suspect everybod\n",
      "> y's\n",
      "> > looking at a big box that has unseen and pseq in it.  Might want to add\n",
      "> > 'pseq' to the 'hide by default' list....\n",
      "> \n",
      "> Was it intended that if you added a sequence to the 'never show' list that\n",
      "> it not take effect till you stopped and restarted exmh?  I added 'pseq',\n",
      "> then hit 'save' for Preferences - didn't take effect till I restarted.\n",
      "\n",
      "No it wasn't, and at one point it worked fine.  I'll check and see why it \n",
      "stopped working.\n",
      "\n",
      "Chris\n",
      "-- \n",
      "Chris Garrigues                 http://www.DeepEddy.Com/~cwg/\n",
      "virCIO                          http://www.virCIO.Com\n",
      "716 Congress, Suite 200\n",
      "Austin, TX  78701\t\t+1 512 374 0500\n",
      "\n",
      "  World War III:  The Wrong-Doers Vs. the Evil-Doers.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Need For Safety Is Real In 2002, You Might Only Get One Chance - Be Ready! Free Shipping & Handling Within The (USA) If You Order Before May 25, 2002! 3 Day Super Sale, Now Until May 7, 2002! Save Up To $30.00 On Some Items! IT'S GETTING TO BE SPRING AGAIN, PROTECT YOURSELF AS YOU WALK, JOG AND EXERCISE OUTSIDE. ALSO PROTECT YOUR LOVED ONES AS THEY RETURN HOME FROM COLLEGE! * LEGAL PROTECTION FOR COLLEGE STUDENTS! * GREAT UP'COMING OUTDOOR PROTECTION GIFTS! * THERE IS NOTHING WORTH MORE PROTECTING THAN LIFE! * OUR STUN DEVICES & PEPPER PRODUCTS ARE LEGAL PROTECTION! JOIN THE WAR ON CRIME! STUN GUNS AND BATONS EFFECTIVE - SAFE - NONLETHAL PROTECT YOUR LOVED ONES AND YOURSELF No matter who you are, no matter what City or Town you live in, if you live in America, you will be touched by crime. You hear about it on TV. You read about it in the newspaper. It's no secret that crime is a major problem in the U.S. today. Criminals are finding it easier to commit crimes all the time. Weapons are readily available. Our cities' police forces have more work than they can handle. Even if these criminal are caught, they won't be spending long in our nation's overcrowded jails. And while lawmakers are well aware of the crime problem, they don't seem to have any effective answers. Our Email Address: Merchants4all@aol.com INTERESTED: You will be protecting yourself within 7 days! Don't Wait, visit our web page below, and join The War On Crime! ***************** http://www.geocities.com/realprotection_20022003/ ***************** Well, there is an effective answer. Take responsibility for your own security. Our site has a variety of quality personal security products. Visit our site, choose the personal security products that are right for you. Use them, and join the war on crime! FREE PEPPER SPRAY WITH ANY STUN UNIT PURCHASE. (A Value of $15.95) We Ship Orders Within 5 To 7 Days, To Every State In The U.S.A. by UPS, FEDEX, or U.S. POSTAL SERVICE. Visa, MasterCard, American Express & Debt Card Gladly Accepted. Ask yourself this question, if you don't help your loved ones, who will? INTERESTED: ***************** http://www.geocities.com/realprotection_20022003/ ***************** ___The Stun Monster 625,000 Volts ($86.95) ___The Z-Force Slim Style 300,000 Volts ($64.95) ___The StunMaster 300,000 Volts Straight ($59.95) ___The StunMaster 300,000 Volts Curb ($59.95) ___The StunMaster 200,000 Volts Straight ($49.95) ___The StunMaster 200,000 Volts Curb ($49.95) ___The StunBaton 500,000 Volts ($89.95) ___The StunBaton 300,000 Volts ($79.95) ___Pen Knife (One $12.50, Two Or More $9.00) ___Wildfire Pepper Spray (One $15.95, Two Or More $11.75) ___Add $5.75 For Shipping & Handling Charge. To Order by postal mail, please send to the below address. Make payable to Mega Safety Technology. Mega Safety Technology 3215 Merrimac Ave. Dayton, Ohio 45405 Our Email Address: Merchants4all@aol.com Order by 24 Hour Fax!!! 775-257-6657. ***** Important Credit Card Information! Please Read Below! * Credit Card Address, City, State and Zip Code, must match billing address to be processed. CHECK____ MONEYORDER____ VISA____ MASTERCARD____ AmericanExpress___ Debt Card___ Name_______________________________________________________ (As it appears on Check or Credit Card) Address____________________________________________________ (As it appears on Check or Credit Card) ___________________________________________________ City,State,Zip(As it appears on Check or Credit Card) ___________________________________________________ Country ___________________________________________________ (Credit Card Number) Expiration Month_____ Year_____ ___________________________________________________ Authorized Signature *****IMPORTANT NOTE***** If Shipping Address Is Different From The Billing Address Above, Please Fill Out Information Below. Shipping Name______________________________________________ Shipping Address___________________________________________ ___________________________________________________________ Shipping City,State,Zip ___________________________________________________________ Country ___________________________________________________________ Email Address & Phone Number(Please Write Neat)\n"
     ]
    }
   ],
   "source": [
    "# General purpose function convert an email to plain text\n",
    "def email_to_text(email):\n",
    "    text_content = \"\"\n",
    "    # Accounts for multi-part emails (reply threads ), checks for text and returns it as plain text \n",
    "    for part in email.walk():\n",
    "        #Finds content type\n",
    "        part_content_type = part.get_content_type()\n",
    "        #Ignores non-text sections\n",
    "        if part_content_type not in ['text/plain', 'text/html']:\n",
    "            continue\n",
    "        #If the section is plain text, strip it out of the email object\n",
    "        if part_content_type == 'text/plain':\n",
    "            text_content += part.get_payload()\n",
    "        #If the section is html, run the html_to_text function\n",
    "        else:\n",
    "            text_content += html_to_text(part)\n",
    "    return text_content\n",
    "\n",
    "#Return email text\n",
    "print(email_to_text(ham_emails[3]))\n",
    "print(email_to_text(spam_emails[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the email to count the word usage in the message\n",
    "class EmailToWordsCount(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strip_headers=True, to_lowercase=True, remove_punc=True, do_stem=True):\n",
    "        self.strip_headers = strip_headers\n",
    "        self.to_lowercase = to_lowercase\n",
    "        self.remove_punc = remove_punc\n",
    "        self.do_stem = do_stem\n",
    "        \n",
    "        # To perform stemming\n",
    "        self.stemmer = nltk.PorterStemmer()\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_word_counts = []\n",
    "        for email in X:\n",
    "            # Text of the email\n",
    "            plain = email_to_text(email)\n",
    "            if plain is None:\n",
    "                plain = \"nothing\"\n",
    "            # Make all letters to lowercase\n",
    "            if self.to_lowercase:\n",
    "                plain = plain.lower()\n",
    "            # Remove all punctuation\n",
    "            if self.remove_punc:\n",
    "                plain = plain.replace(\".\", \"\")\n",
    "                plain = plain.replace(\",\", \"\")\n",
    "                plain = plain.replace(\"!\", \"\")\n",
    "                plain = plain.replace(\"?\", \"\")\n",
    "                plain = plain.replace(\";\", \"\")\n",
    "            # Reduce words to their stems   \n",
    "            word_counts = Counter(plain.split())\n",
    "            if self.do_stem:\n",
    "                # Stem the word, and add their counts\n",
    "                stemmed_word_counts = Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    root_word = self.stemmer.stem(word)\n",
    "                    stemmed_word_counts[root_word] += count\n",
    "                word_counts = stemmed_word_counts\n",
    "            # Count the unique words in a message\n",
    "            X_word_counts.append(word_counts)\n",
    "        return np.array(X_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, today i am go to london for perform and danc "
     ]
    }
   ],
   "source": [
    "#Example of stemming (also from Kaggle Notebook \"Email Spam Classification [98%]\" cited in ReadMe)\n",
    "text = \"Hello, today I am going to London for performing and dancing\"\n",
    "stemmer = nltk.PorterStemmer()\n",
    "\n",
    "for word in text.split():\n",
    "    stemmed_word = stemmer.stem(word)\n",
    "    print(stemmed_word, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a Numpy matrix with the vocabulary of words to consider and their usage counts\n",
    "class WordCountVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "    # Train on list of word counts and build vocabulary\n",
    "    def fit(self, X, y=None):\n",
    "        total_word_counts = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_word_counts[word] += count\n",
    "                \n",
    "        # Build a vocabulary out of total most common\n",
    "        self.most_common = total_word_counts.most_common()[:self.vocabulary_size]\n",
    "        self.vocabulary_ = {word: i for i, (word, count) in enumerate(self.most_common)}\n",
    "    \n",
    "        return self\n",
    "    # Create the vector out of vocabulary\n",
    "    def transform(self, X, y=None):\n",
    "        X_new = np.zeros([X.shape[0], self.vocabulary_size + 1], dtype=int)\n",
    "        \n",
    "        # The vectors will contain additional column for counts of words\n",
    "        # not captured in vocabulary\n",
    "        for row, word_counts in enumerate(X):\n",
    "            for word, count in word_counts.items():\n",
    "                col = self.vocabulary_.get(word, self.vocabulary_size)\n",
    "                X_new[row, col] += count\n",
    "                \n",
    "        return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call both processing functions\n",
    "email_to_cvector = Pipeline([\n",
    "    (\"emailToWords\", EmailToWordsCount()), \n",
    "    (\"wordCountVectorizer\", WordCountVectorizer())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<email.message.Message object at 0x0000021FA0C09540>\n",
      " <email.message.Message object at 0x0000021FA8C04820>\n",
      " <email.message.Message object at 0x0000021FA8C04850> ...\n",
      " <email.message.Message object at 0x0000021FAEA88EB0>\n",
      " <email.message.Message object at 0x0000021FAEA88EE0>\n",
      " <email.message.Message object at 0x0000021FAEA88F10>]\n",
      "[0 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "#Create variables containing the data set and wether each message is spam or ham\n",
    "X = np.array(ham_emails + spam_emails, dtype='object')\n",
    "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n",
    "\n",
    "#Display variables\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Checking data type\n",
    "print(type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (7479,) (7479,)\n",
      "Testing set size:  (1870,) (1870,)\n"
     ]
    }
   ],
   "source": [
    "# Create training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=3301)\n",
    "# Display sizes of sets\n",
    "print(\"Training set size: \", X_train.shape, y_train.shape)\n",
    "print(\"Testing set size: \", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7,   6,   0, ...,   0,   0,  57],\n",
       "       [  9,   7,   0, ...,   0,   0,  42],\n",
       "       [  1,   5,   4, ...,   0,   0,  60],\n",
       "       ...,\n",
       "       [ 14,   9,   0, ...,   0,   0, 134],\n",
       "       [ 33,  20,   0, ...,   1,   0, 125],\n",
       "       [  0,   3,   0, ...,   0,   0,  17]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the training set\n",
    "X_train_prepared = email_to_cvector.fit_transform(X_train)\n",
    "# Display prepared array\n",
    "X_train_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2,   0,   0, ...,   0,   0,  37],\n",
       "       [ 23,  21,   0, ...,   0,   0, 168],\n",
       "       [  2,   3,   0, ...,   0,   0,  19],\n",
       "       ...,\n",
       "       [ 16,   7,   0, ...,   0,   0,  63],\n",
       "       [ 31,  41,  33, ...,   0,   0, 164],\n",
       "       [  0,   4,   0, ...,   0,   0,  22]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the testing set\n",
    "X_test_prepared = email_to_cvector.transform(X_test)\n",
    "# Display prepared array\n",
    "X_test_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that \n",
    "def classification_models(model, X_train_prepared, X_test_prepared, y_train, y_test):\n",
    "    # Trains the model on the training data\n",
    "    model.fit(X_train_prepared, y_train)\n",
    "    # Makes predictions on the testing data\n",
    "    model_prediction = model.predict(X_test_prepared)\n",
    "    # and prints both the classification report and confusion matrix\n",
    "    print(\"Classification Report: \", classification_report(y_test, model_prediction))\n",
    "    matrix = confusion_matrix(y_test, model_prediction)\n",
    "    print(\"Confusion Matrix:\", matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1386\n",
      "           1       0.98      0.96      0.97       484\n",
      "\n",
      "    accuracy                           0.99      1870\n",
      "   macro avg       0.98      0.98      0.98      1870\n",
      "weighted avg       0.99      0.99      0.99      1870\n",
      "\n",
      "Confusion Matrix: [[1378    8]\n",
      " [  19  465]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leean\\anaconda3\\envs\\dev\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1386\n",
      "           1       0.96      0.96      0.96       484\n",
      "\n",
      "    accuracy                           0.98      1870\n",
      "   macro avg       0.98      0.97      0.97      1870\n",
      "weighted avg       0.98      0.98      0.98      1870\n",
      "\n",
      "Confusion Matrix: [[1369   17]\n",
      " [  19  465]]\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98      1386\n",
      "           1       0.95      0.93      0.94       484\n",
      "\n",
      "    accuracy                           0.97      1870\n",
      "   macro avg       0.96      0.95      0.96      1870\n",
      "weighted avg       0.97      0.97      0.97      1870\n",
      "\n",
      "Confusion Matrix: [[1360   26]\n",
      " [  35  449]]\n"
     ]
    }
   ],
   "source": [
    "# List of classification models to evaluate\n",
    "models = [\n",
    "    RandomForestClassifier(random_state=3301),\n",
    "    LogisticRegression(solver=\"lbfgs\", random_state=3301),\n",
    "    DecisionTreeClassifier(random_state=3301)\n",
    "]\n",
    "\n",
    "# Create loop that goes through each model and evals its performance\n",
    "for model in models:\n",
    "    classification_models(model, X_train_prepared, X_test_prepared, y_train, y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note about Confusion Matrix\n",
    "\n",
    "[[True Negative, False Positive][False Negative, True positive]]\n",
    "\n",
    "True Positives (TP): These are the emails that the model correctly identified as spam. This is good because it means the spam filter is doing its job correctly.\n",
    "\n",
    "True Negatives (TN): These are the emails that the model correctly identified as ham (non-spam). This is also good as it indicates that non-spam emails are being correctly identified.\n",
    "\n",
    "False Positives (FP): These are the emails that are actually ham, but the model incorrectly classified them as spam. False positives can be annoying because they may lead to important emails being flagged as spam.\n",
    "\n",
    "False Negatives (FN): These are the emails that are actually spam, but the model incorrectly classified them as ham. False negatives are a more serious issue because they allow spam to reach the inbox, defeating the purpose of the spam filter.\n",
    "\n",
    "##### Explaining model outcomes:\n",
    "\n",
    "1. Random Forest: 99% Accuracy, TP- 465, TN- 1378, FP- 8, FN- 19.\n",
    "\n",
    "2. Logistic Regression: 98% Accuracy, TP- 465, TN- 1369, FP- 17, FN- 19.\n",
    "\n",
    "3. Decision Tree: 97% Accuracy, TP- 449, TN- 1360, FP- 26, FN- 35.\n",
    "\n",
    "The Random Forest Model performed the best and is the model we are choosing to optimize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'bootstrap': True,\n",
      " 'ccp_alpha': 0.0,\n",
      " 'criterion': 'squared_error',\n",
      " 'max_depth': None,\n",
      " 'max_features': 1.0,\n",
      " 'max_leaf_nodes': None,\n",
      " 'max_samples': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': 42,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(random_state = 42)\n",
    "from pprint import pprint\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': [True, False],\n",
      " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
      " 'max_features': ['auto', 'sqrt'],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5, 10],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bootstrap': [True, False],\n",
       " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
       " 'max_features': ['auto', 'sqrt'],\n",
       " 'min_samples_leaf': [1, 2, 4],\n",
       " 'min_samples_split': [2, 5, 10],\n",
       " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a parameter grid to sample from during fitting\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)\n",
    "{'bootstrap': [True, False],\n",
    " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    " 'max_features': ['auto', 'sqrt'],\n",
    " 'min_samples_leaf': [1, 2, 4],\n",
    " 'min_samples_split': [2, 5, 10],\n",
    " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train_prepared, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  View the best params\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
